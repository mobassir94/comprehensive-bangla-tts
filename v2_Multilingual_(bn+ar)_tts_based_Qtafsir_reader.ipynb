{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobassir94/comprehensive-bangla-tts/blob/main/v2_Multilingual_(bn%2Bar)_tts_based_Qtafsir_reader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this notebook we will be using arabic fastspeech2 along with bangla hifi gan vocoder based GlowTTS for multilingual book reading purpose,we will try to read tafsir bayan multilingual (bangla+arabic+english) code mixed tafsir of https://www.hadithbd.com/ with the help of our multilingual TTS pipeline.\n",
        "\n",
        "special thanks to \n",
        "1. [Arabic fastspeech2](https://github.com/ARBML/klaam)\n",
        "2. [Bangla glowtts](https://github.com/Open-Speech-EkStep/vakyansh-tts)"
      ],
      "metadata": {
        "id": "WN2zdHN8Zeby"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZrEmZCJdiSC"
      },
      "source": [
        "Based on our previous [notebook](https://github.com/mobassir94/comprehensive-bangla-tts/blob/main/mlt_TTS_inference_demo/bn-ar-tts-pipeline.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gdown==4.5.1\n",
        "!pip install bnunicodenormalizer==0.1.1\n",
        "# !pip install unidecode\n",
        "# !pip install numpy\n",
        "!pip install pydload==1.0.9\n",
        "!pip install pydub"
      ],
      "metadata": {
        "id": "AgWcwHo-Z5w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and clone necessary files"
      ],
      "metadata": {
        "id": "4iIbbvDFaQbm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3Mm_JHkrFZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28821968-10b1-487e-a070-136389ef630c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vakyansh-tts'...\n",
            "remote: Enumerating objects: 1073, done.\u001b[K\n",
            "remote: Counting objects: 100% (1073/1073), done.\u001b[K\n",
            "remote: Compressing objects: 100% (411/411), done.\u001b[K\n",
            "remote: Total 1073 (delta 650), reused 1070 (delta 649), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1073/1073), 402.46 KiB | 5.09 MiB/s, done.\n",
            "Resolving deltas: 100% (650/650), done.\n",
            "Cloning into 'klaam'...\n",
            "remote: Enumerating objects: 759, done.\u001b[K\n",
            "remote: Counting objects: 100% (584/584), done.\u001b[K\n",
            "remote: Compressing objects: 100% (291/291), done.\u001b[K\n",
            "remote: Total 759 (delta 276), reused 517 (delta 257), pack-reused 175\u001b[K\n",
            "Receiving objects: 100% (759/759), 136.69 MiB | 26.40 MiB/s, done.\n",
            "Resolving deltas: 100% (298/298), done.\n",
            "Checking out files: 100% (175/175), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\n",
        "import gdown\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from IPython.display import Audio\n",
        "import soundfile as sf\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "from pydub import AudioSegment\n",
        "import sys\n",
        "\n",
        "male = True\n",
        "\n",
        "#https://www.kaggle.com/datasets/mobassir/bangla-quran-with-tafsir?select=ben_quran_with_tafsir.csv\n",
        "\n",
        "q_tafsir = 'https://drive.google.com/drive/folders/1MJnVzKRCLzF_GLRkGALbHcLsHrNBl8xV?usp=sharing'\n",
        "gdown.download_folder(url=q_tafsir, quiet=True)  \n",
        "\n",
        "\n",
        "!git clone https://github.com/Open-Speech-EkStep/vakyansh-tts\n",
        "!git clone https://github.com/ARBML/klaam\n",
        "\n",
        "\n",
        "\n",
        "sys.path.append('/content/klaam')\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #need cuda\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "D3cIyItlY0yX",
        "outputId": "bcb08ff8-4250-46e0-97ea-6d1b5360afcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.1+cu113'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqXIsRVfiZ8j",
        "outputId": "926a3b2b-56c9-4186-894d-8a92c2513edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/env/python\n"
          ]
        }
      ],
      "source": [
        "! echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8yj7ivhfn-1"
      },
      "source": [
        "#Bangla GlowTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujNoSjvygKGx"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install bnnumerizer\n",
        "! pip install bangla==0.0.2\n",
        "! pip install Unidecode\n",
        "! pip install numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu4bD9XQgN0K",
        "outputId": "4f28a85d-317d-4a04-b1f5-6ce35e35ed60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-27 08:05:01--  https://storage.googleapis.com/vakyaansh-open-models/translit_models/default_lineup.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.192.128, 209.85.145.128, 209.85.146.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.192.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3422 (3.3K) [application/json]\n",
            "Saving to: ‘default_lineup.json’\n",
            "\n",
            "\rdefault_lineup.json   0%[                    ]       0  --.-KB/s               \rdefault_lineup.json 100%[===================>]   3.34K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-27 08:05:01 (58.5 MB/s) - ‘default_lineup.json’ saved [3422/3422]\n",
            "\n",
            "--2022-10-27 08:05:02--  https://storage.googleapis.com/vakyaansh-open-models/translit_models/bengali/bengali_transliteration.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.192.128, 209.85.145.128, 209.85.146.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.192.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2022-10-27 08:05:02 ERROR 404: Not Found.\n",
            "\n",
            "unzip:  cannot find or open bengali_transliteration, bengali_transliteration.zip or bengali_transliteration.ZIP.\n",
            "male....\n",
            "--2022-10-27 08:05:02--  https://storage.googleapis.com/vakyansh-open-models/tts/bengali/bn-IN/male_voice_1/glow.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.192.128, 209.85.145.128, 209.85.146.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.192.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 310444664 (296M) [application/zip]\n",
            "Saving to: ‘glow.zip’\n",
            "\n",
            "glow.zip            100%[===================>] 296.06M   157MB/s    in 1.9s    \n",
            "\n",
            "2022-10-27 08:05:04 (157 MB/s) - ‘glow.zip’ saved [310444664/310444664]\n",
            "\n",
            "Archive:  glow.zip\n",
            "   creating: glow_ckp/\n",
            "  inflating: glow_ckp/G_200.pth      \n",
            "  inflating: glow_ckp/config.json    \n",
            "--2022-10-27 08:05:08--  https://storage.googleapis.com/vakyansh-open-models/tts/bengali/bn-IN/male_voice_1/hifi.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.192.128, 209.85.145.128, 209.85.146.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.192.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 51761405 (49M) [application/zip]\n",
            "Saving to: ‘hifi.zip’\n",
            "\n",
            "hifi.zip            100%[===================>]  49.36M   132MB/s    in 0.4s    \n",
            "\n",
            "2022-10-27 08:05:08 (132 MB/s) - ‘hifi.zip’ saved [51761405/51761405]\n",
            "\n",
            "Archive:  hifi.zip\n",
            "   creating: hifi_ckp/\n",
            "  inflating: hifi_ckp/g_00100000     \n",
            "  inflating: hifi_ckp/config.json    \n"
          ]
        }
      ],
      "source": [
        "\n",
        "os.chdir('vakyansh-tts') \n",
        "\n",
        "#skipping next 3 line of codes as we have already pytorch latest version installed in this environment\n",
        "\n",
        "# !bash install.sh\n",
        "# !python setup.py bdist_wheel\n",
        "# !pip install -e .\n",
        "\n",
        "os.chdir('tts_infer')\n",
        "!mkdir translit_models\n",
        "os.chdir('translit_models')\n",
        "!wget https://storage.googleapis.com/vakyaansh-open-models/translit_models/default_lineup.json\n",
        "!mkdir bangla\n",
        "os.chdir('bangla')\n",
        "!wget https://storage.googleapis.com/vakyaansh-open-models/translit_models/bengali/bengali_transliteration.zip\n",
        "!unzip bengali_transliteration\n",
        "\n",
        "\n",
        "if(male):\n",
        "  print(\"male....\")\n",
        "  !wget https://storage.googleapis.com/vakyansh-open-models/tts/bengali/bn-IN/male_voice_1/glow.zip\n",
        "  !unzip glow.zip\n",
        "\n",
        "  !wget https://storage.googleapis.com/vakyansh-open-models/tts/bengali/bn-IN/male_voice_1/hifi.zip\n",
        "  !unzip hifi.zip\n",
        "else:\n",
        "  print(\"female....\")\n",
        "  !wget https://storage.googleapis.com/vakyansh-open-models/tts/bengali/bn-IN/female_voice_0/glow.zip\n",
        "  !unzip glow.zip\n",
        "\n",
        "  !wget https://storage.googleapis.com/vakyansh-open-models/tts/bengali/bn-IN/female_voice_0/hifi.zip\n",
        "  !unzip hifi.zip\n",
        "\n",
        "!rm glow.zip\n",
        "!rm hifi.zip\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Annoying print statement during bangla TTS inference"
      ],
      "metadata": {
        "id": "oUCexXP0fRC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/vakyansh-tts/tts_infer/transliterate.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import enum\n",
        "import traceback\n",
        "import re\n",
        "\n",
        "F_DIR = os.path.dirname(os.environ.get('translit_model_base_path', os.path.realpath(__file__)))\n",
        "\n",
        "\n",
        "class XlitError(enum.Enum):\n",
        "    lang_err = \"Unsupported langauge ID requested ;( Please check available languages.\"\n",
        "    string_err = \"String passed is incompatable ;(\"\n",
        "    internal_err = \"Internal crash ;(\"\n",
        "    unknown_err = \"Unknown Failure\"\n",
        "    loading_err = \"Loading failed ;( Check if metadata/paths are correctly configured.\"\n",
        "\n",
        "\n",
        "##=================== Network ==================================================\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        embed_dim,\n",
        "        hidden_dim,\n",
        "        rnn_type=\"gru\",\n",
        "        layers=1,\n",
        "        bidirectional=False,\n",
        "        dropout=0,\n",
        "        device=\"cpu\",\n",
        "    ):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim  # src_vocab_sz\n",
        "        self.enc_embed_dim = embed_dim\n",
        "        self.enc_hidden_dim = hidden_dim\n",
        "        self.enc_rnn_type = rnn_type\n",
        "        self.enc_layers = layers\n",
        "        self.enc_directions = 2 if bidirectional else 1\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(self.input_dim, self.enc_embed_dim)\n",
        "\n",
        "        if self.enc_rnn_type == \"gru\":\n",
        "            self.enc_rnn = nn.GRU(\n",
        "                input_size=self.enc_embed_dim,\n",
        "                hidden_size=self.enc_hidden_dim,\n",
        "                num_layers=self.enc_layers,\n",
        "                bidirectional=bidirectional,\n",
        "            )\n",
        "        elif self.enc_rnn_type == \"lstm\":\n",
        "            self.enc_rnn = nn.LSTM(\n",
        "                input_size=self.enc_embed_dim,\n",
        "                hidden_size=self.enc_hidden_dim,\n",
        "                num_layers=self.enc_layers,\n",
        "                bidirectional=bidirectional,\n",
        "            )\n",
        "        else:\n",
        "            raise Exception(\"XlitError: unknown RNN type mentioned\")\n",
        "\n",
        "    def forward(self, x, x_sz, hidden=None):\n",
        "        \"\"\"\n",
        "        x_sz: (batch_size, 1) -  Unpadded sequence lengths used for pack_pad\n",
        "        \"\"\"\n",
        "        batch_sz = x.shape[0]\n",
        "        # x: batch_size, max_length, enc_embed_dim\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        ## pack the padded data\n",
        "        # x: max_length, batch_size, enc_embed_dim -> for pack_pad\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x, x_sz, enforce_sorted=False)  # unpad\n",
        "\n",
        "        # output: packed_size, batch_size, enc_embed_dim\n",
        "        # hidden: n_layer**num_directions, batch_size, hidden_dim | if LSTM (h_n, c_n)\n",
        "        output, hidden = self.enc_rnn(\n",
        "            x\n",
        "        )  # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
        "\n",
        "        ## pad the sequence to the max length in the batch\n",
        "        # output: max_length, batch_size, enc_emb_dim*directions)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(output)\n",
        "\n",
        "        # output: batch_size, max_length, hidden_dim\n",
        "        output = output.permute(1, 0, 2)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def get_word_embedding(self, x):\n",
        "        \"\"\" \"\"\"\n",
        "        x_sz = torch.tensor([len(x)])\n",
        "        x_ = torch.tensor(x).unsqueeze(0).to(dtype=torch.long)\n",
        "        # x: 1, max_length, enc_embed_dim\n",
        "        x = self.embedding(x_)\n",
        "\n",
        "        ## pack the padded data\n",
        "        # x: max_length, 1, enc_embed_dim -> for pack_pad\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x, x_sz, enforce_sorted=False)  # unpad\n",
        "\n",
        "        # output: packed_size, 1, enc_embed_dim\n",
        "        # hidden: n_layer**num_directions, 1, hidden_dim | if LSTM (h_n, c_n)\n",
        "        output, hidden = self.enc_rnn(\n",
        "            x\n",
        "        )  # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
        "\n",
        "        out_embed = hidden[0].squeeze()\n",
        "\n",
        "        return out_embed\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_dim,\n",
        "        embed_dim,\n",
        "        hidden_dim,\n",
        "        rnn_type=\"gru\",\n",
        "        layers=1,\n",
        "        use_attention=True,\n",
        "        enc_outstate_dim=None,  # enc_directions * enc_hidden_dim\n",
        "        dropout=0,\n",
        "        device=\"cpu\",\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.output_dim = output_dim  # tgt_vocab_sz\n",
        "        self.dec_hidden_dim = hidden_dim\n",
        "        self.dec_embed_dim = embed_dim\n",
        "        self.dec_rnn_type = rnn_type\n",
        "        self.dec_layers = layers\n",
        "        self.use_attention = use_attention\n",
        "        self.device = device\n",
        "        if self.use_attention:\n",
        "            self.enc_outstate_dim = enc_outstate_dim if enc_outstate_dim else hidden_dim\n",
        "        else:\n",
        "            self.enc_outstate_dim = 0\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_dim, self.dec_embed_dim)\n",
        "\n",
        "        if self.dec_rnn_type == \"gru\":\n",
        "            self.dec_rnn = nn.GRU(\n",
        "                input_size=self.dec_embed_dim\n",
        "                + self.enc_outstate_dim,  # to concat attention_output\n",
        "                hidden_size=self.dec_hidden_dim,  # previous Hidden\n",
        "                num_layers=self.dec_layers,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        elif self.dec_rnn_type == \"lstm\":\n",
        "            self.dec_rnn = nn.LSTM(\n",
        "                input_size=self.dec_embed_dim\n",
        "                + self.enc_outstate_dim,  # to concat attention_output\n",
        "                hidden_size=self.dec_hidden_dim,  # previous Hidden\n",
        "                num_layers=self.dec_layers,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        else:\n",
        "            raise Exception(\"XlitError: unknown RNN type mentioned\")\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.dec_hidden_dim, self.dec_embed_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            # nn.Linear(self.dec_embed_dim, self.dec_embed_dim), nn.LeakyReLU(), # removing to reduce size\n",
        "            nn.Linear(self.dec_embed_dim, self.output_dim),\n",
        "        )\n",
        "\n",
        "        ##----- Attention ----------\n",
        "        if self.use_attention:\n",
        "            self.W1 = nn.Linear(self.enc_outstate_dim, self.dec_hidden_dim)\n",
        "            self.W2 = nn.Linear(self.dec_hidden_dim, self.dec_hidden_dim)\n",
        "            self.V = nn.Linear(self.dec_hidden_dim, 1)\n",
        "\n",
        "    def attention(self, x, hidden, enc_output):\n",
        "        \"\"\"\n",
        "        x: (batch_size, 1, dec_embed_dim) -> after Embedding\n",
        "        enc_output: batch_size, max_length, enc_hidden_dim *num_directions\n",
        "        hidden: n_layers, batch_size, hidden_size | if LSTM (h_n, c_n)\n",
        "        \"\"\"\n",
        "\n",
        "        ## perform addition to calculate the score\n",
        "\n",
        "        # hidden_with_time_axis: batch_size, 1, hidden_dim\n",
        "        ## hidden_with_time_axis = hidden.permute(1, 0, 2) ## replaced with below 2lines\n",
        "        hidden_with_time_axis = (\n",
        "            torch.sum(hidden, axis=0)\n",
        "            if self.dec_rnn_type != \"lstm\"\n",
        "            else torch.sum(hidden[0], axis=0)\n",
        "        )  # h_n\n",
        "\n",
        "        hidden_with_time_axis = hidden_with_time_axis.unsqueeze(1)\n",
        "\n",
        "        # score: batch_size, max_length, hidden_dim\n",
        "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "        # attention_weights: batch_size, max_length, 1\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_dim)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        # context_vector: batch_size, 1, hidden_dim\n",
        "        context_vector = context_vector.unsqueeze(1)\n",
        "\n",
        "        # attend_out (batch_size, 1, dec_embed_dim + hidden_size)\n",
        "        attend_out = torch.cat((context_vector, x), -1)\n",
        "\n",
        "        return attend_out, attention_weights\n",
        "\n",
        "    def forward(self, x, hidden, enc_output):\n",
        "        \"\"\"\n",
        "        x: (batch_size, 1)\n",
        "        enc_output: batch_size, max_length, dec_embed_dim\n",
        "        hidden: n_layer, batch_size, hidden_size | lstm: (h_n, c_n)\n",
        "        \"\"\"\n",
        "        if (hidden is None) and (self.use_attention is False):\n",
        "            raise Exception(\n",
        "                \"XlitError: No use of a decoder with No attention and No Hidden\"\n",
        "            )\n",
        "\n",
        "        batch_sz = x.shape[0]\n",
        "\n",
        "        if hidden is None:\n",
        "            # hidden: n_layers, batch_size, hidden_dim\n",
        "            hid_for_att = torch.zeros(\n",
        "                (self.dec_layers, batch_sz, self.dec_hidden_dim)\n",
        "            ).to(self.device)\n",
        "        elif self.dec_rnn_type == \"lstm\":\n",
        "            hid_for_att = hidden[1]  # c_n\n",
        "\n",
        "        # x (batch_size, 1, dec_embed_dim) -> after embedding\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        if self.use_attention:\n",
        "            # x (batch_size, 1, dec_embed_dim + hidden_size) -> after attention\n",
        "            # aw: (batch_size, max_length, 1)\n",
        "            x, aw = self.attention(x, hidden, enc_output)\n",
        "        else:\n",
        "            x, aw = x, 0\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        # output: (batch_size, n_layers, hidden_size)\n",
        "        # hidden: n_layers, batch_size, hidden_size | if LSTM (h_n, c_n)\n",
        "        output, hidden = (\n",
        "            self.dec_rnn(x, hidden) if hidden is not None else self.dec_rnn(x)\n",
        "        )\n",
        "\n",
        "        # output :shp: (batch_size * 1, hidden_size)\n",
        "        output = output.view(-1, output.size(2))\n",
        "\n",
        "        # output :shp: (batch_size * 1, output_dim)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output, hidden, aw\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Class dependency: Encoder, Decoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, encoder, decoder, pass_enc2dec_hid=False, dropout=0, device=\"cpu\"\n",
        "    ):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.pass_enc2dec_hid = pass_enc2dec_hid\n",
        "        _force_en2dec_hid_conv = False\n",
        "\n",
        "        if self.pass_enc2dec_hid:\n",
        "            assert (\n",
        "                decoder.dec_hidden_dim == encoder.enc_hidden_dim\n",
        "            ), \"Hidden Dimension of encoder and decoder must be same, or unset `pass_enc2dec_hid`\"\n",
        "        if decoder.use_attention:\n",
        "            assert (\n",
        "                decoder.enc_outstate_dim\n",
        "                == encoder.enc_directions * encoder.enc_hidden_dim\n",
        "            ), \"Set `enc_out_dim` correctly in decoder\"\n",
        "        assert (\n",
        "            self.pass_enc2dec_hid or decoder.use_attention\n",
        "        ), \"No use of a decoder with No attention and No Hidden from Encoder\"\n",
        "\n",
        "        self.use_conv_4_enc2dec_hid = False\n",
        "        if (\n",
        "            self.pass_enc2dec_hid\n",
        "            and (encoder.enc_directions * encoder.enc_layers != decoder.dec_layers)\n",
        "        ) or _force_en2dec_hid_conv:\n",
        "            if encoder.enc_rnn_type == \"lstm\" or encoder.enc_rnn_type == \"lstm\":\n",
        "                raise Exception(\n",
        "                    \"XlitError: conv for enc2dec_hid not implemented; Change the layer numbers appropriately\"\n",
        "                )\n",
        "\n",
        "            self.use_conv_4_enc2dec_hid = True\n",
        "            self.enc_hid_1ax = encoder.enc_directions * encoder.enc_layers\n",
        "            self.dec_hid_1ax = decoder.dec_layers\n",
        "            self.e2d_hidden_conv = nn.Conv1d(self.enc_hid_1ax, self.dec_hid_1ax, 1)\n",
        "\n",
        "    def enc2dec_hidden(self, enc_hidden):\n",
        "        \"\"\"\n",
        "        enc_hidden: n_layer, batch_size, hidden_dim*num_directions\n",
        "        TODO: Implement the logic for LSTm bsed model\n",
        "        \"\"\"\n",
        "        # hidden: batch_size, enc_layer*num_directions, enc_hidden_dim\n",
        "        hidden = enc_hidden.permute(1, 0, 2).contiguous()\n",
        "        # hidden: batch_size, dec_layers, dec_hidden_dim -> [N,C,Tstep]\n",
        "        hidden = self.e2d_hidden_conv(hidden)\n",
        "\n",
        "        # hidden: dec_layers, batch_size , dec_hidden_dim\n",
        "        hidden_for_dec = hidden.permute(1, 0, 2).contiguous()\n",
        "\n",
        "        return hidden_for_dec\n",
        "\n",
        "    def active_beam_inference(self, src, beam_width=3, max_tgt_sz=50):\n",
        "        \"\"\"Search based decoding\n",
        "        src: (sequence_len)\n",
        "        \"\"\"\n",
        "\n",
        "        def _avg_score(p_tup):\n",
        "            \"\"\"Used for Sorting\n",
        "            TODO: Dividing by length of sequence power alpha as hyperparam\n",
        "            \"\"\"\n",
        "            return p_tup[0]\n",
        "\n",
        "        import sys\n",
        "\n",
        "        batch_size = 1\n",
        "        start_tok = src[0]\n",
        "        end_tok = src[-1]\n",
        "        src_sz = torch.tensor([len(src)])\n",
        "        src_ = src.unsqueeze(0)\n",
        "\n",
        "        # enc_output: (batch_size, padded_seq_length, enc_hidden_dim*num_direction)\n",
        "        # enc_hidden: (enc_layers*num_direction, batch_size, hidden_dim)\n",
        "        enc_output, enc_hidden = self.encoder(src_, src_sz)\n",
        "\n",
        "        if self.pass_enc2dec_hid:\n",
        "            # dec_hidden: dec_layers, batch_size , dec_hidden_dim\n",
        "            if self.use_conv_4_enc2dec_hid:\n",
        "                init_dec_hidden = self.enc2dec_hidden(enc_hidden)\n",
        "            else:\n",
        "                init_dec_hidden = enc_hidden\n",
        "        else:\n",
        "            # dec_hidden -> Will be initialized to zeros internally\n",
        "            init_dec_hidden = None\n",
        "\n",
        "        # top_pred[][0] = Σ-log_softmax\n",
        "        # top_pred[][1] = sequence torch.tensor shape: (1)\n",
        "        # top_pred[][2] = dec_hidden\n",
        "        top_pred_list = [(0, start_tok.unsqueeze(0), init_dec_hidden)]\n",
        "\n",
        "        for t in range(max_tgt_sz):\n",
        "            cur_pred_list = []\n",
        "\n",
        "            for p_tup in top_pred_list:\n",
        "                if p_tup[1][-1] == end_tok:\n",
        "                    cur_pred_list.append(p_tup)\n",
        "                    continue\n",
        "\n",
        "                # dec_hidden: dec_layers, 1, hidden_dim\n",
        "                # dec_output: 1, output_dim\n",
        "                dec_output, dec_hidden, _ = self.decoder(\n",
        "                    x=p_tup[1][-1].view(1, 1),  # dec_input: (1,1)\n",
        "                    hidden=p_tup[2],\n",
        "                    enc_output=enc_output,\n",
        "                )\n",
        "\n",
        "                ## π{prob} = Σ{log(prob)} -> to prevent diminishing\n",
        "                # dec_output: (1, output_dim)\n",
        "                dec_output = nn.functional.log_softmax(dec_output, dim=1)\n",
        "                # pred_topk.values & pred_topk.indices: (1, beam_width)\n",
        "                pred_topk = torch.topk(dec_output, k=beam_width, dim=1)\n",
        "\n",
        "                for i in range(beam_width):\n",
        "                    sig_logsmx_ = p_tup[0] + pred_topk.values[0][i]\n",
        "                    # seq_tensor_ : (seq_len)\n",
        "                    seq_tensor_ = torch.cat((p_tup[1], pred_topk.indices[0][i].view(1)))\n",
        "\n",
        "                    cur_pred_list.append((sig_logsmx_, seq_tensor_, dec_hidden))\n",
        "\n",
        "            cur_pred_list.sort(key=_avg_score, reverse=True)  # Maximized order\n",
        "            top_pred_list = cur_pred_list[:beam_width]\n",
        "\n",
        "            # check if end_tok of all topk\n",
        "            end_flags_ = [1 if t[1][-1] == end_tok else 0 for t in top_pred_list]\n",
        "            if beam_width == sum(end_flags_):\n",
        "                break\n",
        "\n",
        "        pred_tnsr_list = [t[1] for t in top_pred_list]\n",
        "\n",
        "        return pred_tnsr_list\n",
        "\n",
        "\n",
        "##===================== Glyph handlers =======================================\n",
        "\n",
        "\n",
        "class GlyphStrawboss:\n",
        "    def __init__(self, glyphs=\"en\"):\n",
        "        \"\"\"list of letters in a language in unicode\n",
        "        lang: ISO Language code\n",
        "        glyphs: json file with script information\n",
        "        \"\"\"\n",
        "        if glyphs == \"en\":\n",
        "            # Smallcase alone\n",
        "            self.glyphs = [chr(alpha) for alpha in range(97, 122 + 1)]\n",
        "        else:\n",
        "            self.dossier = json.load(open(glyphs, encoding=\"utf-8\"))\n",
        "            self.glyphs = self.dossier[\"glyphs\"]\n",
        "            self.numsym_map = self.dossier[\"numsym_map\"]\n",
        "\n",
        "        self.char2idx = {}\n",
        "        self.idx2char = {}\n",
        "        self._create_index()\n",
        "\n",
        "    def _create_index(self):\n",
        "\n",
        "        self.char2idx[\"_\"] = 0  # pad\n",
        "        self.char2idx[\"$\"] = 1  # start\n",
        "        self.char2idx[\"#\"] = 2  # end\n",
        "        self.char2idx[\"*\"] = 3  # Mask\n",
        "        self.char2idx[\"'\"] = 4  # apostrophe U+0027\n",
        "        self.char2idx[\"%\"] = 5  # unused\n",
        "        self.char2idx[\"!\"] = 6  # unused\n",
        "\n",
        "        # letter to index mapping\n",
        "        for idx, char in enumerate(self.glyphs):\n",
        "            self.char2idx[char] = idx + 7  # +7 token initially\n",
        "\n",
        "        # index to letter mapping\n",
        "        for char, idx in self.char2idx.items():\n",
        "            self.idx2char[idx] = char\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.char2idx)\n",
        "\n",
        "    def word2xlitvec(self, word):\n",
        "        \"\"\"Converts given string of gyphs(word) to vector(numpy)\n",
        "        Also adds tokens for start and end\n",
        "        \"\"\"\n",
        "        try:\n",
        "            vec = [self.char2idx[\"$\"]]  # start token\n",
        "            for i in list(word):\n",
        "                vec.append(self.char2idx[i])\n",
        "            vec.append(self.char2idx[\"#\"])  # end token\n",
        "\n",
        "            vec = np.asarray(vec, dtype=np.int64)\n",
        "            return vec\n",
        "\n",
        "        except Exception as error:\n",
        "            print(\"XlitError: In word:\", word, \"Error Char not in Token:\", error)\n",
        "            sys.exit()\n",
        "\n",
        "    def xlitvec2word(self, vector):\n",
        "        \"\"\"Converts vector(numpy) to string of glyphs(word)\"\"\"\n",
        "        char_list = []\n",
        "        for i in vector:\n",
        "            char_list.append(self.idx2char[i])\n",
        "\n",
        "        word = \"\".join(char_list).replace(\"$\", \"\").replace(\"#\", \"\")  # remove tokens\n",
        "        word = word.replace(\"_\", \"\").replace(\"*\", \"\")  # remove tokens\n",
        "        return word\n",
        "\n",
        "\n",
        "class VocabSanitizer:\n",
        "    def __init__(self, data_file):\n",
        "        \"\"\"\n",
        "        data_file: path to file conatining vocabulary list\n",
        "        \"\"\"\n",
        "        extension = os.path.splitext(data_file)[-1]\n",
        "        if extension == \".json\":\n",
        "            self.vocab_set = set(json.load(open(data_file, encoding=\"utf-8\")))\n",
        "        elif extension == \".csv\":\n",
        "            self.vocab_df = pd.read_csv(data_file).set_index(\"WORD\")\n",
        "            self.vocab_set = set(self.vocab_df.index)\n",
        "        else:\n",
        "            print(\"XlitError: Only Json/CSV file extension supported\")\n",
        "\n",
        "    def reposition(self, word_list):\n",
        "        \"\"\"Reorder Words in list\"\"\"\n",
        "        new_list = []\n",
        "        temp_ = word_list.copy()\n",
        "        for v in word_list:\n",
        "            if v in self.vocab_set:\n",
        "                new_list.append(v)\n",
        "                temp_.remove(v)\n",
        "        new_list.extend(temp_)\n",
        "\n",
        "        return new_list\n",
        "\n",
        "\n",
        "##=============== INSTANTIATION ================================================\n",
        "\n",
        "\n",
        "class XlitPiston:\n",
        "    \"\"\"\n",
        "    For handling prediction & post-processing of transliteration for a single language\n",
        "    Class dependency: Seq2Seq, GlyphStrawboss, VocabSanitizer\n",
        "    Global Variables: F_DIR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        weight_path,\n",
        "        vocab_file,\n",
        "        tglyph_cfg_file,\n",
        "        iglyph_cfg_file=\"en\",\n",
        "        device=\"cpu\",\n",
        "    ):\n",
        "\n",
        "        self.device = device\n",
        "        self.in_glyph_obj = GlyphStrawboss(iglyph_cfg_file)\n",
        "        self.tgt_glyph_obj = GlyphStrawboss(glyphs=tglyph_cfg_file)\n",
        "        self.voc_sanity = VocabSanitizer(vocab_file)\n",
        "\n",
        "        self._numsym_set = set(\n",
        "            json.load(open(tglyph_cfg_file, encoding=\"utf-8\"))[\"numsym_map\"].keys()\n",
        "        )\n",
        "        self._inchar_set = set(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "        self._natscr_set = set().union(\n",
        "            self.tgt_glyph_obj.glyphs, sum(self.tgt_glyph_obj.numsym_map.values(), [])\n",
        "        )\n",
        "\n",
        "        ## Model Config Static                TODO: add defining in json support\n",
        "        input_dim = self.in_glyph_obj.size()\n",
        "        output_dim = self.tgt_glyph_obj.size()\n",
        "        enc_emb_dim = 300\n",
        "        dec_emb_dim = 300\n",
        "        enc_hidden_dim = 512\n",
        "        dec_hidden_dim = 512\n",
        "        rnn_type = \"lstm\"\n",
        "        enc2dec_hid = True\n",
        "        attention = True\n",
        "        enc_layers = 1\n",
        "        dec_layers = 2\n",
        "        m_dropout = 0\n",
        "        enc_bidirect = True\n",
        "        enc_outstate_dim = enc_hidden_dim * (2 if enc_bidirect else 1)\n",
        "\n",
        "        enc = Encoder(\n",
        "            input_dim=input_dim,\n",
        "            embed_dim=enc_emb_dim,\n",
        "            hidden_dim=enc_hidden_dim,\n",
        "            rnn_type=rnn_type,\n",
        "            layers=enc_layers,\n",
        "            dropout=m_dropout,\n",
        "            device=self.device,\n",
        "            bidirectional=enc_bidirect,\n",
        "        )\n",
        "        dec = Decoder(\n",
        "            output_dim=output_dim,\n",
        "            embed_dim=dec_emb_dim,\n",
        "            hidden_dim=dec_hidden_dim,\n",
        "            rnn_type=rnn_type,\n",
        "            layers=dec_layers,\n",
        "            dropout=m_dropout,\n",
        "            use_attention=attention,\n",
        "            enc_outstate_dim=enc_outstate_dim,\n",
        "            device=self.device,\n",
        "        )\n",
        "        self.model = Seq2Seq(enc, dec, pass_enc2dec_hid=enc2dec_hid, device=self.device)\n",
        "        self.model = self.model.to(self.device)\n",
        "        weights = torch.load(weight_path, map_location=torch.device(self.device))\n",
        "\n",
        "        self.model.load_state_dict(weights)\n",
        "        self.model.eval()\n",
        "\n",
        "    def character_model(self, word, beam_width=1):\n",
        "        in_vec = torch.from_numpy(self.in_glyph_obj.word2xlitvec(word)).to(self.device)\n",
        "        ## change to active or passive beam\n",
        "        p_out_list = self.model.active_beam_inference(in_vec, beam_width=beam_width)\n",
        "        p_result = [\n",
        "            self.tgt_glyph_obj.xlitvec2word(out.cpu().numpy()) for out in p_out_list\n",
        "        ]\n",
        "\n",
        "        result = self.voc_sanity.reposition(p_result)\n",
        "\n",
        "        # List type\n",
        "        return result\n",
        "\n",
        "    def numsym_model(self, seg):\n",
        "        \"\"\"tgt_glyph_obj.numsym_map[x] returns a list object\"\"\"\n",
        "        if len(seg) == 1:\n",
        "            return [seg] + self.tgt_glyph_obj.numsym_map[seg]\n",
        "\n",
        "        a = [self.tgt_glyph_obj.numsym_map[n][0] for n in seg]\n",
        "        return [seg] + [\"\".join(a)]\n",
        "\n",
        "    def _word_segementer(self, sequence):\n",
        "\n",
        "        sequence = sequence.lower()\n",
        "        accepted = set().union(self._numsym_set, self._inchar_set, self._natscr_set)\n",
        "        # sequence = ''.join([i for i in sequence if i in accepted])\n",
        "\n",
        "        segment = []\n",
        "        idx = 0\n",
        "        seq_ = list(sequence)\n",
        "        while len(seq_):\n",
        "            # for Number-Symbol\n",
        "            temp = \"\"\n",
        "            while len(seq_) and seq_[0] in self._numsym_set:\n",
        "                temp += seq_[0]\n",
        "                seq_.pop(0)\n",
        "            if temp != \"\":\n",
        "                segment.append(temp)\n",
        "\n",
        "            # for Target Chars\n",
        "            temp = \"\"\n",
        "            while len(seq_) and seq_[0] in self._natscr_set:\n",
        "                temp += seq_[0]\n",
        "                seq_.pop(0)\n",
        "            if temp != \"\":\n",
        "                segment.append(temp)\n",
        "\n",
        "            # for Input-Roman Chars\n",
        "            temp = \"\"\n",
        "            while len(seq_) and seq_[0] in self._inchar_set:\n",
        "                temp += seq_[0]\n",
        "                seq_.pop(0)\n",
        "            if temp != \"\":\n",
        "                segment.append(temp)\n",
        "\n",
        "            temp = \"\"\n",
        "            while len(seq_) and seq_[0] not in accepted:\n",
        "                temp += seq_[0]\n",
        "                seq_.pop(0)\n",
        "            if temp != \"\":\n",
        "                segment.append(temp)\n",
        "\n",
        "        return segment\n",
        "\n",
        "    def inferencer(self, sequence, beam_width=10):\n",
        "\n",
        "        seg = self._word_segementer(sequence[:120])\n",
        "        lit_seg = []\n",
        "\n",
        "        p = 0\n",
        "        while p < len(seg):\n",
        "            if seg[p][0] in self._natscr_set:\n",
        "                lit_seg.append([seg[p]])\n",
        "                p += 1\n",
        "\n",
        "            elif seg[p][0] in self._inchar_set:\n",
        "                lit_seg.append(self.character_model(seg[p], beam_width=beam_width))\n",
        "                p += 1\n",
        "\n",
        "            elif seg[p][0] in self._numsym_set:  # num & punc\n",
        "                lit_seg.append(self.numsym_model(seg[p]))\n",
        "                p += 1\n",
        "            else:\n",
        "                lit_seg.append([seg[p]])\n",
        "                p += 1\n",
        "\n",
        "        ## IF segment less/equal to 2 then return combinotorial,\n",
        "        ## ELSE only return top1 of each result concatenated\n",
        "        if len(lit_seg) == 1:\n",
        "            final_result = lit_seg[0]\n",
        "\n",
        "        elif len(lit_seg) == 2:\n",
        "            final_result = [\"\"]\n",
        "            for seg in lit_seg:\n",
        "                new_result = []\n",
        "                for s in seg:\n",
        "                    for f in final_result:\n",
        "                        new_result.append(f + s)\n",
        "                final_result = new_result\n",
        "\n",
        "        else:\n",
        "            new_result = []\n",
        "            for seg in lit_seg:\n",
        "                new_result.append(seg[0])\n",
        "            final_result = [\"\".join(new_result)]\n",
        "\n",
        "        return final_result\n",
        "\n",
        "\n",
        "from collections.abc import Iterable\n",
        "from pydload import dload\n",
        "import zipfile\n",
        "\n",
        "MODEL_DOWNLOAD_URL_PREFIX = \"https://github.com/AI4Bharat/IndianNLP-Transliteration/releases/download/xlit_v0.5.0/\"\n",
        "\n",
        "\n",
        "def is_folder_writable(folder):\n",
        "    try:\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "        tmp_file = os.path.join(folder, \".write_test\")\n",
        "        with open(tmp_file, \"w\") as f:\n",
        "            f.write(\"Permission Check\")\n",
        "        os.remove(tmp_file)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "\n",
        "def is_directory_writable(path):\n",
        "    if os.name == \"nt\":\n",
        "        return is_folder_writable(path)\n",
        "    return os.access(path, os.W_OK | os.X_OK)\n",
        "\n",
        "\n",
        "class XlitEngine:\n",
        "    \"\"\"\n",
        "    For Managing the top level tasks and applications of transliteration\n",
        "    Global Variables: F_DIR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, lang2use=\"all\", config_path=\"translit_models/default_lineup.json\"\n",
        "    ):\n",
        "\n",
        "        lineup = json.load(open(os.path.join(F_DIR, config_path), encoding=\"utf-8\"))\n",
        "        self.lang_config = {}\n",
        "        if isinstance(lang2use, str):\n",
        "            if lang2use == \"all\":\n",
        "                self.lang_config = lineup\n",
        "            elif lang2use in lineup:\n",
        "                self.lang_config[lang2use] = lineup[lang2use]\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"XlitError: The entered Langauge code not found. Available are {}\".format(\n",
        "                        lineup.keys()\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        elif isinstance(lang2use, Iterable):\n",
        "            for l in lang2use:\n",
        "                try:\n",
        "                    self.lang_config[l] = lineup[l]\n",
        "                except:\n",
        "                    print(\n",
        "                        \"XlitError: Language code {} not found, Skipping...\".format(l)\n",
        "                    )\n",
        "        else:\n",
        "            raise Exception(\n",
        "                \"XlitError: lang2use must be a list of language codes (or) string of single language code\"\n",
        "            )\n",
        "\n",
        "        if is_directory_writable(F_DIR):\n",
        "            models_path = os.path.join(F_DIR, \"translit_models\")\n",
        "        else:\n",
        "            user_home = os.path.expanduser(\"~\")\n",
        "            models_path = os.path.join(user_home, \".AI4Bharat_Xlit_Models\")\n",
        "        os.makedirs(models_path, exist_ok=True)\n",
        "        self.download_models(models_path)\n",
        "\n",
        "        self.langs = {}\n",
        "        self.lang_model = {}\n",
        "        for la in self.lang_config:\n",
        "            try:\n",
        "                #print(\"Loading {}...\".format(la))\n",
        "                self.lang_model[la] = XlitPiston(\n",
        "                    weight_path=os.path.join(\n",
        "                        models_path, self.lang_config[la][\"weight\"]\n",
        "                    ),\n",
        "                    vocab_file=os.path.join(models_path, self.lang_config[la][\"vocab\"]),\n",
        "                    tglyph_cfg_file=os.path.join(\n",
        "                        models_path, self.lang_config[la][\"script\"]\n",
        "                    ),\n",
        "                    iglyph_cfg_file=\"en\",\n",
        "                )\n",
        "                self.langs[la] = self.lang_config[la][\"name\"]\n",
        "            except Exception as error:\n",
        "                print(\"XlitError: Failure in loading {} \\n\".format(la), error)\n",
        "                print(XlitError.loading_err.value)\n",
        "\n",
        "    def download_models(self, models_path):\n",
        "        \"\"\"\n",
        "        Download models from GitHub Releases if not exists\n",
        "        \"\"\"\n",
        "        for l in self.lang_config:\n",
        "            lang_name = self.lang_config[l][\"eng_name\"]\n",
        "            lang_model_path = os.path.join(models_path, lang_name)\n",
        "            if not os.path.isdir(lang_model_path):\n",
        "                print(\"Downloading model for language: %s\" % lang_name)\n",
        "                remote_url = MODEL_DOWNLOAD_URL_PREFIX + lang_name + \".zip\"\n",
        "                downloaded_zip_path = os.path.join(models_path, lang_name + \".zip\")\n",
        "                dload(url=remote_url, save_to_path=downloaded_zip_path, max_time=None)\n",
        "\n",
        "                if not os.path.isfile(downloaded_zip_path):\n",
        "                    exit(\n",
        "                        f\"ERROR: Unable to download model from {remote_url} into {models_path}\"\n",
        "                    )\n",
        "\n",
        "                with zipfile.ZipFile(downloaded_zip_path, \"r\") as zip_ref:\n",
        "                    zip_ref.extractall(models_path)\n",
        "\n",
        "                if os.path.isdir(lang_model_path):\n",
        "                    os.remove(downloaded_zip_path)\n",
        "                else:\n",
        "                    exit(\n",
        "                        f\"ERROR: Unable to find models in {lang_model_path} after download\"\n",
        "                    )\n",
        "        return\n",
        "\n",
        "    def translit_word(self, eng_word, lang_code=\"default\", topk=7, beam_width=10):\n",
        "        if eng_word == \"\":\n",
        "            return []\n",
        "\n",
        "        if lang_code in self.langs:\n",
        "            try:\n",
        "                res_list = self.lang_model[lang_code].inferencer(\n",
        "                    eng_word, beam_width=beam_width\n",
        "                )\n",
        "                return res_list[:topk]\n",
        "\n",
        "            except Exception as error:\n",
        "                print(\"XlitError:\", traceback.format_exc())\n",
        "                print(XlitError.internal_err.value)\n",
        "                return XlitError.internal_err\n",
        "\n",
        "        elif lang_code == \"default\":\n",
        "            try:\n",
        "                res_dict = {}\n",
        "                for la in self.lang_model:\n",
        "                    res = self.lang_model[la].inferencer(\n",
        "                        eng_word, beam_width=beam_width\n",
        "                    )\n",
        "                    res_dict[la] = res[:topk]\n",
        "                return res_dict\n",
        "\n",
        "            except Exception as error:\n",
        "                print(\"XlitError:\", traceback.format_exc())\n",
        "                print(XlitError.internal_err.value)\n",
        "                return XlitError.internal_err\n",
        "\n",
        "        else:\n",
        "            print(\"XlitError: Unknown Langauge requested\", lang_code)\n",
        "            print(XlitError.lang_err.value)\n",
        "            return XlitError.lang_err\n",
        "\n",
        "    def translit_sentence(self, eng_sentence, lang_code=\"default\", beam_width=10):\n",
        "        if eng_sentence == \"\":\n",
        "            return []\n",
        "\n",
        "        if lang_code in self.langs:\n",
        "            try:\n",
        "                out_str = \"\"\n",
        "                for word in eng_sentence.split():\n",
        "                    res_ = self.lang_model[lang_code].inferencer(\n",
        "                        word, beam_width=beam_width\n",
        "                    )\n",
        "                    out_str = out_str + res_[0] + \" \"\n",
        "                return out_str[:-1]\n",
        "\n",
        "            except Exception as error:\n",
        "                print(\"XlitError:\", traceback.format_exc())\n",
        "                print(XlitError.internal_err.value)\n",
        "                return XlitError.internal_err\n",
        "\n",
        "        elif lang_code == \"default\":\n",
        "            try:\n",
        "                res_dict = {}\n",
        "                for la in self.lang_model:\n",
        "                    out_str = \"\"\n",
        "                    for word in eng_sentence.split():\n",
        "                        res_ = self.lang_model[la].inferencer(\n",
        "                            word, beam_width=beam_width\n",
        "                        )\n",
        "                        out_str = out_str + res_[0] + \" \"\n",
        "                    res_dict[la] = out_str[:-1]\n",
        "                return res_dict\n",
        "\n",
        "            except Exception as error:\n",
        "                print(\"XlitError:\", traceback.format_exc())\n",
        "                print(XlitError.internal_err.value)\n",
        "                return XlitError.internal_err\n",
        "\n",
        "        else:\n",
        "            print(\"XlitError: Unknown Langauge requested\", lang_code)\n",
        "            print(XlitError.lang_err.value)\n",
        "            return XlitError.lang_err\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    available_lang = [\n",
        "        \"bn\",\n",
        "        \"gu\",\n",
        "        \"hi\",\n",
        "        \"kn\",\n",
        "        \"gom\",\n",
        "        \"mai\",\n",
        "        \"ml\",\n",
        "        \"mr\",\n",
        "        \"pa\",\n",
        "        \"sd\",\n",
        "        \"si\",\n",
        "        \"ta\",\n",
        "        \"te\",\n",
        "        \"ur\",\n",
        "    ]\n",
        "\n",
        "    reg = re.compile(r\"[a-zA-Z]\")\n",
        "    lang = \"hi\"\n",
        "    engine = XlitEngine(\n",
        "        lang\n",
        "    )  # if you don't specify lang code here, this will give results in all langs available\n",
        "    sent = \"Hello World! ABCD क्या हाल है आपका?\"\n",
        "    words = [\n",
        "        engine.translit_word(word, topk=1)[lang][0] if reg.match(word) else word\n",
        "        for word in sent.split()\n",
        "    ]  # only transliterated en words, leaves rest as it is\n",
        "    updated_sent = \" \".join(words)\n",
        "\n",
        "    print(updated_sent)\n",
        "\n",
        "    # output : हेलो वर्ल्ड! क्या हाल है आपका?\n",
        "\n",
        "    # y = engine.translit_sentence(\"Hello World !\")['hi']\n",
        "    # print(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87e9FamOfVvz",
        "outputId": "9266a4bf-bb40-44f2-9471-761a9adfdfcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/vakyansh-tts/tts_infer/transliterate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bengali TTS inference demo"
      ],
      "metadata": {
        "id": "5rjvzGrSZFSo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2Lmmj2LkikW",
        "outputId": "6f50f8fd-f699-4704-cbe9-bab648f93fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/vakyansh-tts/tts_infer/translit_models/bangla/glow_ckp/G_200.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/vakyansh-tts/tts_infer/../src/glow_tts/modules.py:234: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
            "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
            "Q, R = torch.qr(A, some)\n",
            "should be replaced with\n",
            "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2491.)\n",
            "  w_init = torch.qr(torch.FloatTensor(self.n_split, self.n_split).normal_())[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/vakyansh-tts/tts_infer/translit_models/bangla/hifi_ckp/g_00100000\n",
            "Loading '/content/vakyansh-tts/tts_infer/translit_models/bangla/hifi_ckp/g_00100000'\n",
            "Complete.\n",
            "Removing weight norm...\n"
          ]
        }
      ],
      "source": [
        "# os.chdir('/content/vakyansh-tts/')\n",
        "\n",
        "sys.path.append('/content/vakyansh-tts')\n",
        "import bangla\n",
        "from tts_infer.tts import TextToMel, MelToWav\n",
        "from tts_infer.transliterate import XlitEngine\n",
        "from tts_infer.num_to_word_on_sent import normalize_nums\n",
        "\n",
        "import re\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "text_to_mel = TextToMel(glow_model_dir='/content/vakyansh-tts/tts_infer/translit_models/bangla/glow_ckp', device=device)\n",
        "mel_to_wav = MelToWav(hifi_model_dir='/content/vakyansh-tts/tts_infer/translit_models/bangla/hifi_ckp', device=device)\n",
        "\n",
        "def translit(text, lang):\n",
        "    reg = re.compile(r'[a-zA-Z]')\n",
        "    engine = XlitEngine(lang)\n",
        "    words = [engine.translit_word(word, topk=1)[lang][0] if reg.match(word) else word for word in text.split()]\n",
        "    updated_sent = ' '.join(words)\n",
        "    return updated_sent\n",
        "    \n",
        "def run_tts(text, lang):\n",
        "    # text = text.replace('।', '.') # only for hindi models\n",
        "    text_num_to_word = normalize_nums(text, lang) # converting numbers to words in lang\n",
        "    text_num_to_word_and_transliterated = translit(text_num_to_word, lang) # transliterating english words to lang\n",
        "    \n",
        "    mel = text_to_mel.generate_mel(text_num_to_word_and_transliterated)\n",
        "    audio, sr = mel_to_wav.generate_wav(mel)\n",
        "    write(filename='temp.wav', rate=sr, data=audio) # for saving wav file, if needed\n",
        "    return (sr, audio)\n",
        "\n",
        "def ben_tts(text, lang):\n",
        "    # text = text.replace('।', '.') # only for hindi models\n",
        "    text_num_to_word = normalize_nums(text, lang) # converting numbers to words in lang\n",
        "    text_num_to_word_and_transliterated = translit(text_num_to_word, lang) # transliterating english words to lang\n",
        "    \n",
        "    mel = text_to_mel.generate_mel(text_num_to_word_and_transliterated)\n",
        "    audio, sr = mel_to_wav.generate_wav(mel)\n",
        "    del mel,text_num_to_word_and_transliterated,text_num_to_word\n",
        "    return audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp0CFgiB4Qmj",
        "outputId": "f529f051-264e-465a-bd96-0cd67562fe9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/vakyansh-tts/tts_infer/translit_models/bangla\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H04CZokf4XUh"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8-OkXjt4c-3",
        "outputId": "ff3ed306-adbc-439d-f903-33061719ede4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iYwzIjw4hVZ"
      },
      "outputs": [],
      "source": [
        "_, audio = run_tts('বিসমিল্লাহ’র পূর্বে ‘আক্বরাউ’ ‘আবদাউ’ অথবা ‘আতলু’ ফে’ল ক্রিয়া উহ্য আছে। অর্থাৎ, আল্লাহর নাম নিয়ে পড়ছি  কিংবা তেলাঅত আরম্ভ করছি। ', 'bn')\n",
        "Audio('temp.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77oyc6v6tqN2",
        "outputId": "2f482e06-f9a7-4dfe-be06-392981b53325",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 27 08:05:29 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    33W /  70W |   1202MiB / 15109MiB |     98%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q1x8pX72Dr_"
      },
      "source": [
        "# Arabic FastSpeech2 TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlcT_Oaz2W4T"
      },
      "outputs": [],
      "source": [
        "\n",
        "os.chdir('klaam') \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyarabic\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "35nnBukRTfV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content') \n",
        "from klaam import TextToSpeech\n",
        "from pyarabic.araby import strip_diacritics\n",
        "root_path = \"/content/klaam/\"\n",
        "prepare_tts_model_path = \"/content/klaam/cfgs/FastSpeech2/config/Arabic/preprocess.yaml\"\n",
        "model_config_path = \"/content/klaam/cfgs/FastSpeech2/config/Arabic/model.yaml\"\n",
        "train_config_path = \"/content/klaam/cfgs/FastSpeech2/config/Arabic/train.yaml\"\n",
        "vocoder_config_path = \"/content/klaam/cfgs/FastSpeech2/model_config/hifigan/config.json\"\n",
        "speaker_pre_trained_path = \"/content/klaam/data/model_weights/hifigan/generator_universal.pth.tar\"\n",
        "\n",
        "ar_model = TextToSpeech(prepare_tts_model_path, model_config_path, train_config_path, vocoder_config_path, speaker_pre_trained_path,root_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv0IY0fSTofb",
        "outputId": "fe2084b1-93dc-482f-ecae-e740598d674c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1J7ZP_q-6mryXUhZ-8j9-RIItz2nJGOIX\n",
            "To: /content/model.pth.tar\n",
            "100%|██████████| 418M/418M [00:02<00:00, 191MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing weight norm...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xFdUJ7zpMVm"
      },
      "outputs": [],
      "source": [
        "text = \"ٱلۡحَمۡدُ لِلَّهِ رَبِّ ٱلۡعَٰلَمِينَ\"\n",
        "\n",
        "ar_model.synthesize(text)\n",
        "\n",
        "#sf.write('sound.wav', wave, 22050)\n",
        "Audio('./sample.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mP4Zpl5gUMe"
      },
      "source": [
        "#TTS BigTextToAudio\n",
        "\n",
        "long text to audio conversion "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGykRN4wgYIp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torchaudio.functional as F\n",
        "import torchaudio\n",
        "from bnnumerizer import numerize\n",
        "import gc\n",
        "from bnunicodenormalizer import Normalizer \n",
        "# initialize\n",
        "bnorm=Normalizer()\n",
        "\n",
        "# Create empty audio file of 1 second duration (purpose -> post processing)\n",
        "\n",
        "audio = AudioSegment.silent(duration=1000)\n",
        "sound = audio.set_frame_rate(audio.frame_rate*2)\n",
        "sound.export(\"./empty.wav\", format=\"wav\")\n",
        "\n",
        "#loading empty audio file of  1 second to append before and after each arabic chunk for increasing mlt reading rhythm.\n",
        "\n",
        "empty_audio, rate_of_sample = torchaudio.load('/content/empty.wav')\n",
        "empty_audio = empty_audio.flatten()\n",
        "\n",
        "def normalize(sen):\n",
        "    _words = [bnorm(word)['normalized']  for word in sen.split()]\n",
        "    return \" \".join([word for word in _words if word is not None]) \n",
        "\n",
        "class BigTextToAudio(object):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 ar_model,\n",
        "                 bn_model,\n",
        "                 ar_sample_rate=22050,\n",
        "                 bn_sample_rate=22050,\n",
        "                 out_sample_rate=22050,\n",
        "                 attribution_dict={\"সাঃ\":\"সাল্লাল্লাহু আলাইহি ওয়া সাল্লাম\",                   \n",
        "                                  \"আঃ\":\"আলাইহিস সালাম\",\n",
        "                                  \"রাঃ\":\"রাদিআল্লাহু আনহু\",\n",
        "                                  \"রহঃ\":\"রহমাতুল্লাহি আলাইহি\",\n",
        "                                  \"রহিঃ\":\"রহিমাহুল্লাহ\",\n",
        "                                  \"হাফিঃ\":\"হাফিযাহুল্লাহ\",\n",
        "                                  \"বায়ান\":\"বাইআন\",\n",
        "                                  \"দাঃবাঃ\":\"দামাত বারাকাতুহুম,দামাত বারাকাতুল্লাহ\",\n",
        "                                  \"আয়াত\" : \"আইআত\",#আইআত\n",
        "                                  # \"ওয়া\" : \"ওআ\",\n",
        "                                  # \"ওয়াসাল্লাম\"  : \"ওআসাল্লাম\",\n",
        "                                  # \"কেন\"  : \"কেনো\",\n",
        "                                  # \"কোন\" : \"কোনো\",\n",
        "                                  # \"বল\"   : \"বলো\",\n",
        "                                  # \"চল\"   : \"চলো\",\n",
        "                                  # \"কর\"   : \"করো\",\n",
        "                                  # \"রাখ\"   : \"রাখো\",\n",
        "                                  \"’\"     :  \"\",\n",
        "                                  \"‘\"     : \"\",\n",
        "                                  # \"য়\"     : \"অ\",\n",
        "                                  # \"সম্প্রদায়\" : \"সম্প্রদাই\",\n",
        "                                  # \"রয়েছে\"   : \"রইছে\",\n",
        "                                  # \"রয়েছ\"    : \"রইছ\",\n",
        "                                   \"/\"   : \" বাই \",\n",
        "                                  },\n",
        "                resample_params={\"lowpass_filter_width\": 64,\n",
        "                                \"rolloff\": 0.9475937167399596,\n",
        "                                \"resampling_method\": \"kaiser_window\",\n",
        "                                \"beta\": 14.769656459379492}\n",
        "                ):\n",
        "        '''\n",
        "            Instantiates a Big Text to Audio conversion object for bangla and arabic\n",
        "            args:\n",
        "                ar_model : arabic tts model\n",
        "                bn_model : bangla tts model\n",
        "                ar_sample_rate : arabic audio sample rate [optional] default: 22050\n",
        "                bn_sample_rate : bangla audio sample rate [optional] default: 22050\n",
        "                out_sample_rate : audio sample rate [optional] default: 22050\n",
        "                attribution_dict : a dict for attribution expansion [optional]\n",
        "                resample_params : audio resampling parameters [optional]\n",
        "            resources:\n",
        "                # Main class: modified from https://github.com/snakers4/silero-models/pull/174\n",
        "                # Audio converter:https://www.kaggle.com/code/shahruk10/inference-notebook-wav2vec2\n",
        "        '''\n",
        "        self.ar_model = ar_model\n",
        "        self.bn_model = bn_model\n",
        "        self.attribution_dict=attribution_dict\n",
        " \n",
        "        self.ar_sample_rate=ar_sample_rate\n",
        "        self.bn_sample_rate=bn_sample_rate\n",
        "        self.sample_rate=out_sample_rate  \n",
        "        self.resample_params=resample_params\n",
        "        \n",
        "    # public\n",
        "    def ar_tts(self,text):\n",
        "        '''\n",
        "            args: \n",
        "                text: arabic text (string)\n",
        "            returns:\n",
        "                audio as torch tensor\n",
        "        '''\n",
        "        text = strip_diacritics(text)\n",
        "\n",
        "        try:\n",
        "          self.ar_model.synthesize(text)\n",
        "          audio, rate_of_sample = torchaudio.load('/content/sample.wav')\n",
        "          audio = audio.flatten()\n",
        "          audio = torch.cat([empty_audio,audio], axis=0) #start empty\n",
        "          audio = torch.cat([audio,empty_audio], axis=0) #end empty\n",
        "          audio = audio * 32768.0\n",
        "        except:\n",
        "            print(\"failed ar =>\",text,\"end\")\n",
        "            audio = empty_audio * 32768.0\n",
        "                  \n",
        "        \n",
        "        return audio\n",
        "    # public\n",
        "    def bn_tts(self,text):\n",
        "        '''\n",
        "            args: \n",
        "                text   : bangla text (string)\n",
        "            returns:\n",
        "                audio as torch tensor\n",
        "        '''\n",
        "        if(self.bn_model == 'glowtts'):\n",
        "          audio = ben_tts(text, 'bn')\n",
        "          #audio = torch.from_numpy(audio)\n",
        "\n",
        "        else:\n",
        "          print(\"not supported!\")\n",
        "\n",
        "        return torch.tensor(audio)\n",
        "    \n",
        "    # public\n",
        "    def expand_full_attribution(self,text):\n",
        "        for word in self.attribution_dict:\n",
        "            if word in text:\n",
        "                text = text.replace(word, normalize(self.attribution_dict[word]))\n",
        "        return text\n",
        "    \n",
        "    def collapse_whitespace(self,text):\n",
        "        # Regular expression matching whitespace:\n",
        "        _whitespace_re = re.compile(r\"\\s+\")\n",
        "        return re.sub(_whitespace_re, \" \", text)\n",
        "\n",
        "    # public\n",
        "    def tag_text(self,text):\n",
        "        '''\n",
        "            * tags arabic text with <ar>text</ar>\n",
        "            * tags bangla text with <bn>text</bn>\n",
        "        '''\n",
        "        # remove multiple spaces\n",
        "        text=re.sub(' +', ' ',text)\n",
        "        # create start and end\n",
        "        text=\"start\"+text+\"end\"\n",
        "        # tag text\n",
        "        parts=re.split(u'[\\u0600-\\u06FF]+', text)\n",
        "        # remove non chars\n",
        "        parts=[p for p in parts if p.strip()]\n",
        "        # unique parts\n",
        "        parts=set(parts)\n",
        "        # tag the text\n",
        "        for m in parts:\n",
        "            if len(m.strip())>1:text=text.replace(m,f\"</ar><SPLIT><bn>{m}</bn><SPLIT><ar>\")\n",
        "        # clean-tags\n",
        "        text=text.replace(\"</ar><SPLIT><bn>start\",'<bn>')\n",
        "        text=text.replace(\"end</bn><SPLIT><ar>\",'</bn>')\n",
        "        return text\n",
        "\n",
        "    def process_text(self,text):\n",
        "        '''\n",
        "        process multilingual text for suitable MLT TTS format\n",
        "            * expand attributions\n",
        "            * numerize text\n",
        "            * tag sections of the text\n",
        "            * sequentially list text blocks\n",
        "            * Split based on sentence ending Characters\n",
        "\n",
        "        '''\n",
        "        \n",
        "        # english numbers to bangla conversion\n",
        "        res = re.search('[0-9]', text)\n",
        "        if res is not None:\n",
        "          text = bangla.convert_english_digit_to_bangla_digit(text)\n",
        "        \n",
        "        #replace ':' in between two bangla numbers with ' এর '\n",
        "        pattern=r\"[০, ১, ২, ৩, ৪, ৫, ৬, ৭, ৮, ৯]:[০, ১, ২, ৩, ৪, ৫, ৬, ৭, ৮, ৯]\"\n",
        "        matches=re.findall(pattern,text)\n",
        "        for m in matches:\n",
        "            r=m.replace(\":\",\" এর \")\n",
        "            text=text.replace(m,r)\n",
        "\n",
        "        # numerize text\n",
        "        text=numerize(text)\n",
        "\n",
        "        # tag sections\n",
        "        text=self.tag_text(text)\n",
        "        # text blocks\n",
        "        blocks=text.split(\"<SPLIT>\")\n",
        "        blocks=[b for b in blocks if b.strip()]\n",
        "        # create tuple of (lang,text)\n",
        "        data=[]\n",
        "        for block in blocks:\n",
        "            lang=None\n",
        "            if \"<bn>\" in block:\n",
        "                block=block.replace(\"<bn>\",'').replace(\"</bn>\",'')\n",
        "                lang=\"bn\"\n",
        "            elif \"<ar>\" in block:\n",
        "                block=block.replace(\"<ar>\",'').replace(\"</ar>\",'')\n",
        "                lang=\"ar\"\n",
        "            \n",
        "            # Split based on sentence ending Characters\n",
        "\n",
        "            if lang == \"bn\":\n",
        "              bn_text = block.strip()\n",
        "\n",
        "              sentenceEnders = re.compile('[।!?]')\n",
        "              sentences = sentenceEnders.split(str(bn_text))\n",
        "\n",
        "              for i in range(len(sentences)):\n",
        "                  res = re.sub('\\n','',sentences[i])\n",
        "                  res = normalize(res)\n",
        "                  # expand attributes\n",
        "                  res=self.expand_full_attribution(res)\n",
        "\n",
        "                  res = self.collapse_whitespace(res)\n",
        "                  res += '।'\n",
        "                  data.append((lang,res))\n",
        "\n",
        "            elif lang == \"ar\":\n",
        "                ar_text = block.strip()\n",
        "                ar_text = re.sub(\"؟\", \"?\", ar_text) # replace any ؟ with ?\n",
        "\n",
        "                sentenceEnders = re.compile('[.!?]')\n",
        "                sentences = sentenceEnders.split(str(ar_text))\n",
        "\n",
        "                for i in range(len(sentences)):\n",
        "                    res = re.sub('\\n','',sentences[i])\n",
        "                    res = self.collapse_whitespace(res)\n",
        "                    data.append((lang,res))\n",
        "                    \n",
        "        return data\n",
        "    \n",
        "    def resample_audio(self,audio,sr):\n",
        "        '''\n",
        "            resample audio with sample rate\n",
        "            args:\n",
        "                audio : torch.tensor audio\n",
        "                sr: audi sample rate\n",
        "        '''\n",
        "        if sr==self.sample_rate:\n",
        "            return audio\n",
        "        else:\n",
        "            return F.resample(audio,sr,self.sample_rate,**self.resample_params)\n",
        "        \n",
        "    \n",
        "    def get_audio(self,data):\n",
        "        '''\n",
        "            creates audio from given data \n",
        "                * data=List[Tuples(lang,text)]\n",
        "        '''\n",
        "        audio_list = []\n",
        "        for block in data:\n",
        "            lang,text=block\n",
        "            if lang==\"bn\":\n",
        "                audio=self.bn_tts(text)\n",
        "                sr=self.bn_sample_rate\n",
        "            else:\n",
        "                audio=self.ar_tts(text)\n",
        "                sr=self.ar_sample_rate\n",
        "            \n",
        "            if self.resample_audio_to_out_sample_rate:\n",
        "                audio=self.resample_audio(audio,sr)\n",
        "                \n",
        "            audio_list.append(audio)\n",
        "  \n",
        "        audio = torch.cat([k for k in audio_list])\n",
        "        return audio\n",
        "    \n",
        "    # call\n",
        "    def __call__(self,text,resample_audio_to_out_sample_rate=True):\n",
        "        '''\n",
        "            args: \n",
        "                text   : bangla text (string)\n",
        "                resample_audio_to_out_sample_rate: for different sample rate in different models, resample the output audio \n",
        "                                                   in uniform sample rate \n",
        "                                                   * default:True\n",
        "            returns:\n",
        "                audio as numpy data\n",
        "        '''\n",
        "        self.resample_audio_to_out_sample_rate=resample_audio_to_out_sample_rate\n",
        "        data=self.process_text(text)\n",
        "        audio=self.get_audio(data)\n",
        "        return audio.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5_wZr-5g3b3"
      },
      "source": [
        "#check textprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTnf2Xt3g2mo"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv('/content/bn_qtafsir/ben_quran_with_tafsir.csv')\n",
        "text = df.tafsir_bayan_text[1]\n",
        "print(text)\n",
        "bn_model = 'glowtts'\n",
        "MLT_TTS=BigTextToAudio(ar_model=ar_model,\n",
        "                   bn_model=bn_model)\n",
        "\n",
        "data=MLT_TTS.process_text(text)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GePBfBhdglfv"
      },
      "outputs": [],
      "source": [
        "\n",
        "audio=MLT_TTS(text,resample_audio_to_out_sample_rate=True)\n",
        "Audio(audio, rate=MLT_TTS.sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sf.write(f'./check_mlt.wav', audio/ 32768.0, 22050)\n",
        "#Audio('/content/check_mlt.wav', autoplay=True)"
      ],
      "metadata": {
        "id": "kcstEOGH3CGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3Ufm4g_zoBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9e5e39c-c95d-4fbd-f5e1-68e5bf43bcfd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6236"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "len(df.tafsir_bayan_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mlt_reader(surah_list,df):\n",
        "    '''\n",
        "      surah_list -> list of surah's for what we will be launching MLT TTS\n",
        "      df         -> dataframe containing all bangla quranic verses and tafsir\n",
        "    '''\n",
        "    surah_curr=surah_list[0]\n",
        "    bn_audio = []\n",
        "    for i in range(len(df)):\n",
        "        surah = df.ayat[i].split(\",\")[0]\n",
        "        surah_num = int(re.findall(r'\\d+', surah)[0])\n",
        "        \n",
        "        if(surah_num not in surah_list):\n",
        "            continue\n",
        "        else:\n",
        "            bn_verse = df.ayat[i].replace(\"surah\",\"সুরা\").replace(\"ayat\",\"আয়াত\")\n",
        "            bn_verse = bangla.convert_english_digit_to_bangla_digit(bn_verse)\n",
        "            try:\n",
        "              bn_qtafsir = bn_verse +\" এর বাংলা অনুবাদ পরছি। \"+ df.আল_বায়ান[i] + \" । \" + \" আয়াত টির বাংলা তাফসির পরছি। \"+df.tafsir_bayan_text[i] \n",
        "            except:\n",
        "              #debug\n",
        "              print(\"bn_verse \",bn_verse,\"আল_বায়ান \",df.আল_বায়ান[i],\"df.tafsir_bayan_text[i]  \\n\\n\",df.tafsir_bayan_text[i] )\n",
        "            audio=MLT_TTS(bn_qtafsir,resample_audio_to_out_sample_rate=True)\n",
        "            # if current surah is bein\n",
        "            if surah_curr==surah_num:    \n",
        "                bn_audio.append(audio)\n",
        "\n",
        "                full_surah_audio=np.concatenate(bn_audio,axis=-1)\n",
        "                sf.write(f'./{surah_num}.wav', full_surah_audio/ 32768.0, 22050)\n",
        "            else:\n",
        "                bn_audio=[]\n",
        "                surah_curr=surah_num"
      ],
      "metadata": {
        "id": "N4eQBwvfOIgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.replace(np.nan, 'এটির তাফসির নেই।', regex=True)"
      ],
      "metadata": {
        "id": "9lMePwiBjZgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "surah_list = [1]\n",
        "mlt_reader(surah_list = surah_list,df = df)"
      ],
      "metadata": {
        "id": "oiJt4wprV3ZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b47375-b1be-4091-ee6e-81c70f0a0057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipped\n",
            "['<', 'a', 'l', 'H', 'm', 'd']\n",
            "Raw Text Sequence: الحمد\n",
            "Phoneme Sequence: {< a l H m d sil}\n",
            "Raw Text Sequence: ال\n",
            "Phoneme Sequence: {< a l sil}\n",
            "Raw Text Sequence: استغراق\n",
            "Phoneme Sequence: {s t g r AA q sil}\n",
            "Raw Text Sequence: اختصاص\n",
            "Phoneme Sequence: {x t S AA S sil}\n",
            "skipped\n",
            "['r', 'b']\n",
            "Raw Text Sequence: رب\n",
            "Phoneme Sequence: {r b sil}\n",
            "Raw Text Sequence: عالمين عالم\n",
            "Phoneme Sequence: {E aa l m ii0 n E aa l m sil}\n",
            "Raw Text Sequence: عالم\n",
            "Phoneme Sequence: {E aa l m sil}\n",
            "Raw Text Sequence: رحما\n",
            "Phoneme Sequence: {r H m aa sil}\n",
            "Raw Text Sequence: فعلان\n",
            "Phoneme Sequence: {f E l aa n sil}\n",
            "Raw Text Sequence: رحيم\n",
            "Phoneme Sequence: {r H ii0 m sil}\n",
            "Raw Text Sequence: فعيل\n",
            "Phoneme Sequence: {f E ii0 l sil}\n",
            "skipped\n",
            "['<', 'a', 'l', 'l', 'h', 'm']\n",
            "Raw Text Sequence: اللهم\n",
            "Phoneme Sequence: {< a l l h m sil}\n",
            "failed ar =>  اجعلنا منهم end\n",
            "skipped\n",
            "['t', 'm', 'l', 'k']\n",
            "skipped\n",
            "['n', 'f', 's']\n",
            "skipped\n",
            "['l', 'n', 'f', 's']\n",
            "skipped\n",
            "['w', 'a', 'l', '<', 'm', 'r']\n",
            "skipped\n",
            "['y', 'uu0', 'm', '<', '*']\n",
            "skipped\n",
            "['l', 'l', 'h']\n",
            "Raw Text Sequence: يوم لا تملك نفس لنفس شيئا والأمر يومئذ لله\n",
            "Phoneme Sequence: {y uu0 m l aa t m l k n f s l n f s $ ii0 < aa w a l < m r y uu0 m < * l l h sil}\n",
            "skipped\n",
            "['n', 'E', 'b', 'd', 'k']\n",
            "skipped\n",
            "['uu0', 'n', 's', 't', 'E', 'ii0', 'n', 'k']\n",
            "Raw Text Sequence: نعبدك ونستعينك\n",
            "Phoneme Sequence: {n E b d k uu0 n s t E ii0 n k sil}\n",
            "Raw Text Sequence: مفعول\n",
            "Phoneme Sequence: {m f E uu0 l sil}\n",
            "skipped\n",
            "['f', 'E', 'l']\n",
            "Raw Text Sequence: فعل\n",
            "Phoneme Sequence: {f E l sil}\n",
            "skipped\n",
            "['n', 'E', 'b', 'd']\n",
            "skipped\n",
            "['uu0', '<', 'i0', 'y', 'aa', 'k']\n",
            "Raw Text Sequence: إياك نعبد وإياك نستعين\n",
            "Phoneme Sequence: {< i0 y aa k n E b d uu0 < i0 y aa k n s t E ii0 n sil}\n",
            "skipped\n",
            "['m', 'n']\n",
            "skipped\n",
            "['l', 'l', 'h']\n",
            "Raw Text Sequence: من أنصاري إلى الله\n",
            "Phoneme Sequence: {m n < a n S AA r ii0 < i0 l aa l l h sil}\n",
            "skipped\n",
            "['uu0', 't', 'E', 'aa', 'uu0', 'n', 'uu0']\n",
            "skipped\n",
            "['l', 'b', 'r']\n",
            "Raw Text Sequence: وتعاونوا على البر والتقوى\n",
            "Phoneme Sequence: {uu0 t E aa uu0 n uu0 E l aa l b r w a l t q w aa sil}\n",
            "skipped\n",
            "['l', 'l', 'h']\n",
            "skipped\n",
            "['m', 'n', 'h']\n",
            "Raw Text Sequence: أعاذنا الله منه\n",
            "Phoneme Sequence: {< a E aa * n aa l l h m n h sil}\n",
            "skipped\n",
            "['l', 'l', 'h']\n",
            "skipped\n",
            "['m', 'n', 'h']\n",
            "Raw Text Sequence: أعاذنا الله منه\n",
            "Phoneme Sequence: {< a E aa * n aa l l h m n h sil}\n",
            "Raw Text Sequence: اهدنا\n",
            "Phoneme Sequence: {h d n aa sil}\n",
            "Raw Text Sequence: مغضوب عليهم\n",
            "Phoneme Sequence: {m g D uu0 b E l ii0 h m sil}\n",
            "Raw Text Sequence: ضالين\n",
            "Phoneme Sequence: {D AA l ii0 n sil}\n",
            "Raw Text Sequence: المغضوب عليهم\n",
            "Phoneme Sequence: {< a l m g D uu0 b E l ii0 h m sil}\n",
            "Raw Text Sequence: الضالين\n",
            "Phoneme Sequence: {< a l D AA l ii0 n sil}\n",
            "Raw Text Sequence: كذلك فليكن\n",
            "Phoneme Sequence: {k a * aa l i1 k f l ii0 k n sil}\n",
            "Raw Text Sequence: لا تخيب رجآءنا\n",
            "Phoneme Sequence: {l aa t x ii0 b r j < aa < n aa sil}\n",
            "skipped\n",
            "['<', 'a', 'l', 'l', 'h', 'm']\n",
            "skipped\n",
            "['s', 't', 'j', 'b']\n",
            "Raw Text Sequence: اللهم استجب لنا\n",
            "Phoneme Sequence: {< a l l h m s t j b l n aa sil}\n",
            "CPU times: user 2min 8s, sys: 23.8 s, total: 2min 32s\n",
            "Wall time: 2min 33s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "jkpoSBG7FfJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa41b8c-ab2b-4d7e-9778-1c1ffb8212a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Audio \n",
        "# Audio('/content/1.wav', autoplay=True)"
      ],
      "metadata": {
        "id": "v2UH2jklI9Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gWCLilr_2P9i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}